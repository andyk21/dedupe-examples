{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import optparse\n",
    "import locale\n",
    "import itertools\n",
    "import io\n",
    "import csv\n",
    "import dj_database_url\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import dedupe\n",
    "import numpy\n",
    "\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "\n",
    "register_adapter(numpy.int32, AsIs)\n",
    "register_adapter(numpy.int64, AsIs)\n",
    "register_adapter(numpy.float32, AsIs)\n",
    "register_adapter(numpy.float64, AsIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Methods/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Readable(object):\n",
    "\n",
    "    def __init__(self, iterator):\n",
    "\n",
    "        self.output = io.StringIO()\n",
    "        self.writer = csv.writer(self.output)\n",
    "        self.iterator = iterator\n",
    "\n",
    "    def read(self, size):\n",
    "\n",
    "        self.writer.writerows(itertools.islice(self.iterator, size))\n",
    "\n",
    "        chunk = self.output.getvalue()\n",
    "        self.output.seek(0)\n",
    "        self.output.truncate(0)\n",
    "\n",
    "        return chunk\n",
    "    \n",
    "def record_pairs(result_set):\n",
    "\n",
    "    for i, row in enumerate(result_set):\n",
    "        a_record_id, a_record, b_record_id, b_record = row\n",
    "        record_a = (a_record_id, a_record)\n",
    "        record_b = (b_record_id, b_record)\n",
    "\n",
    "        yield record_a, record_b\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "def cluster_ids(clustered_dupes):\n",
    "\n",
    "    for cluster, scores in clustered_dupes:\n",
    "        cluster_id = cluster[0]\n",
    "        for donor_id, score in zip(cluster, scores):\n",
    "            yield donor_id, cluster_id, score            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control verbosity\n",
    "verbose = 1\n",
    "\n",
    "if verbose == 1:\n",
    "    log_level = logging.INFO\n",
    "elif verbose >= 2:\n",
    "    log_level = logging.DEBUG\n",
    "    \n",
    "logging.getLogger().setLevel(log_level)\n",
    "\n",
    "# Preexisting settings/training file\n",
    "settings_file = 'pgsql_big_dedupe_example_settings'\n",
    "training_file = 'pgsql_big_dedupe_example_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATABASE_URL=postgres://test:testpassword@localhost/dedupe-example\n"
     ]
    }
   ],
   "source": [
    "# set environment variable DATABASE_URL\n",
    "# template: %env DATABASE_URL=postgres://{user}:{password}@{host}/{db-name}\n",
    "%env DATABASE_URL=postgres://test:testpassword@localhost/dedupe-example\n",
    "\n",
    "# Connect to DB\n",
    "db_conf = dj_database_url.config()\n",
    "\n",
    "if not db_conf:\n",
    "    raise Exception(\n",
    "        'set DATABASE_URL environment variable with your connection, e.g. '\n",
    "        'export DATABASE_URL=postgres://user:password@host/mydatabase'\n",
    "    )\n",
    "    \n",
    "read_con = psycopg2.connect(database=db_conf['NAME'],\n",
    "                            user=db_conf['USER'],\n",
    "                            password=db_conf['PASSWORD'],\n",
    "                            host=db_conf['HOST'],\n",
    "                            cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "write_con = psycopg2.connect(database=db_conf['NAME'],\n",
    "                             user=db_conf['USER'],\n",
    "                             password=db_conf['PASSWORD'],\n",
    "                             host=db_conf['HOST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [{'field': 'name', 'type': 'String'},\n",
    "          {'field': 'address', 'type': 'String', 'has missing': True},\n",
    "          {'field': 'city', 'type': 'ShortString', 'has missing': True},\n",
    "          {'field': 'state', 'type': 'ShortString', 'has missing': True},\n",
    "          {'field': 'zip', 'type': 'ShortString', 'has missing': True},\n",
    "         ]\n",
    "\n",
    "# Initialize Deduper with given fields\n",
    "deduper = dedupe.Dedupe(fields, num_cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query\n",
    "DONOR_SELECT = \"SELECT donor_id, city, name, zip, state, address FROM processed_donors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. rows: 706030\n"
     ]
    }
   ],
   "source": [
    "# Read from 'processed_donors'\n",
    "with read_con.cursor('donor_select') as cur:\n",
    "    cur.execute(DONOR_SELECT)\n",
    "    temp_d = {i: row for i, row in enumerate(cur)}\n",
    "    \n",
    "    # example element of temp_d:\n",
    "    # RealDictRow([('donor_id', 435),\n",
    "    #         ('city', None),\n",
    "    #         ('name', '12-19-02 cash deposit'),\n",
    "    #         ('zip', None),\n",
    "    #         ('state', 'il'),\n",
    "    #        ('address', None)])\n",
    "\n",
    "with read_con.cursor('count') as cur:\n",
    "    cur.execute('SELECT COUNT(*) FROM processed_donors')\n",
    "    print('Num. rows', cur.fetchone()['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:reading training from file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading labeled examples from  pgsql_big_dedupe_example_training.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (wholeFieldPredicate, address), SimplePredicate: (wholeFieldPredicate, name))\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (doubleMetaphone, name), SimplePredicate: (sortedAcronym, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonThreeTokens, name), SimplePredicate: (hundredIntegerPredicate, address))\n",
      "INFO:dedupe.training:(SimplePredicate: (metaphoneToken, name), SimplePredicate: (twoGramFingerprint, address))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonIntegerPredicate, name), SimplePredicate: (commonThreeTokens, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (doubleMetaphone, address), SimplePredicate: (sortedAcronym, name))\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(training_file):\n",
    "    print('reading labeled examples from ', training_file)\n",
    "    with open(training_file) as tf:\n",
    "        deduper.prepare_training(temp_d, tf)\n",
    "else:\n",
    "    deduper.prepare_training(temp_d)\n",
    "    \n",
    "del temp_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name : iuoe local #399-political educ. fund\n",
      "address : 2260 so. grove st.\n",
      "city : chicago\n",
      "state : il\n",
      "zip : 60616\n",
      "\n",
      "name : iuoe local 399\n",
      "address : 2260 s grove str\n",
      "city : chicago\n",
      "state : il\n",
      "zip : 60616\n",
      "\n",
      "32/10 positive, 2/10 negative\n",
      "Do these records refer to the same thing?\n",
      "(y)es / (n)o / (u)nsure / (f)inished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished labeling\n"
     ]
    }
   ],
   "source": [
    "dedupe.console_label(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_file, 'w') as tf:\n",
    "    deduper.write_training(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rlr.crossvalidation:using cross validation to find optimum alpha...\n",
      "/usr/local/anaconda3/envs/dedupe/lib/python3.7/site-packages/rlr/crossvalidation.py:122: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  * (true_distinct + false_distinct)))\n",
      "INFO:rlr.crossvalidation:optimum alpha: 0.000010, score 0.3043999749933167\n",
      "INFO:dedupe.training:Final predicate set:\n",
      "INFO:dedupe.training:(SimplePredicate: (doubleMetaphone, name), SimplePredicate: (sortedAcronym, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonThreeTokens, address), SimplePredicate: (commonThreeTokens, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (firstTokenPredicate, name), SimplePredicate: (twoGramFingerprint, address))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonIntegerPredicate, name), SimplePredicate: (commonThreeTokens, name))\n",
      "INFO:dedupe.training:(SimplePredicate: (commonTwoTokens, city), SimplePredicate: (oneGramFingerprint, name))\n"
     ]
    }
   ],
   "source": [
    "deduper.train(recall=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(settings_file, 'wb') as sf:\n",
    "    deduper.write_settings(sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduper.cleanup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in deduper.fingerprinter.index_fields:\n",
    "    print(field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating inverted index\n",
      "writing blocking map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.blocking:10000, 0.7684602 seconds\n",
      "INFO:dedupe.blocking:20000, 1.5433532 seconds\n",
      "INFO:dedupe.blocking:30000, 2.2997902 seconds\n",
      "INFO:dedupe.blocking:40000, 3.0500072 seconds\n",
      "INFO:dedupe.blocking:50000, 3.8522632 seconds\n",
      "INFO:dedupe.blocking:60000, 4.6934402 seconds\n",
      "INFO:dedupe.blocking:70000, 5.6468152 seconds\n",
      "INFO:dedupe.blocking:80000, 6.4750682 seconds\n",
      "INFO:dedupe.blocking:90000, 7.2550532 seconds\n",
      "INFO:dedupe.blocking:100000, 7.9991922 seconds\n",
      "INFO:dedupe.blocking:110000, 8.7456752 seconds\n",
      "INFO:dedupe.blocking:120000, 9.5421292 seconds\n",
      "INFO:dedupe.blocking:130000, 10.2824442 seconds\n",
      "INFO:dedupe.blocking:140000, 11.0973792 seconds\n",
      "INFO:dedupe.blocking:150000, 11.9101712 seconds\n",
      "INFO:dedupe.blocking:160000, 12.6880932 seconds\n",
      "INFO:dedupe.blocking:170000, 13.5542102 seconds\n",
      "INFO:dedupe.blocking:180000, 14.3557012 seconds\n",
      "INFO:dedupe.blocking:190000, 15.1254132 seconds\n",
      "INFO:dedupe.blocking:200000, 15.9005712 seconds\n",
      "INFO:dedupe.blocking:210000, 16.7706892 seconds\n",
      "INFO:dedupe.blocking:220000, 17.5153242 seconds\n",
      "INFO:dedupe.blocking:230000, 18.2655252 seconds\n",
      "INFO:dedupe.blocking:240000, 19.0501592 seconds\n",
      "INFO:dedupe.blocking:250000, 19.8272942 seconds\n",
      "INFO:dedupe.blocking:260000, 20.6001262 seconds\n",
      "INFO:dedupe.blocking:270000, 21.3454862 seconds\n",
      "INFO:dedupe.blocking:280000, 22.1167492 seconds\n",
      "INFO:dedupe.blocking:290000, 22.8929062 seconds\n",
      "INFO:dedupe.blocking:300000, 23.6999752 seconds\n",
      "INFO:dedupe.blocking:310000, 24.4542852 seconds\n",
      "INFO:dedupe.blocking:320000, 25.2678002 seconds\n",
      "INFO:dedupe.blocking:330000, 26.0510522 seconds\n",
      "INFO:dedupe.blocking:340000, 26.8021912 seconds\n",
      "INFO:dedupe.blocking:350000, 27.5158352 seconds\n",
      "INFO:dedupe.blocking:360000, 28.1127032 seconds\n",
      "INFO:dedupe.blocking:370000, 28.7230122 seconds\n",
      "INFO:dedupe.blocking:380000, 29.3229652 seconds\n",
      "INFO:dedupe.blocking:390000, 29.9491182 seconds\n",
      "INFO:dedupe.blocking:400000, 30.5683562 seconds\n",
      "INFO:dedupe.blocking:410000, 31.1497912 seconds\n",
      "INFO:dedupe.blocking:420000, 31.7779382 seconds\n",
      "INFO:dedupe.blocking:430000, 32.4048102 seconds\n",
      "INFO:dedupe.blocking:440000, 33.0613842 seconds\n",
      "INFO:dedupe.blocking:450000, 33.7020782 seconds\n",
      "INFO:dedupe.blocking:460000, 34.3191322 seconds\n",
      "INFO:dedupe.blocking:470000, 34.9598552 seconds\n",
      "INFO:dedupe.blocking:480000, 35.5664352 seconds\n",
      "INFO:dedupe.blocking:490000, 36.2033252 seconds\n",
      "INFO:dedupe.blocking:500000, 36.8200432 seconds\n",
      "INFO:dedupe.blocking:510000, 37.4677242 seconds\n",
      "INFO:dedupe.blocking:520000, 38.1495032 seconds\n",
      "INFO:dedupe.blocking:530000, 38.8200502 seconds\n",
      "INFO:dedupe.blocking:540000, 39.4868252 seconds\n",
      "INFO:dedupe.blocking:550000, 40.1238762 seconds\n",
      "INFO:dedupe.blocking:560000, 40.7588392 seconds\n",
      "INFO:dedupe.blocking:570000, 41.4382712 seconds\n",
      "INFO:dedupe.blocking:580000, 42.0831872 seconds\n",
      "INFO:dedupe.blocking:590000, 42.7948252 seconds\n",
      "INFO:dedupe.blocking:600000, 43.4687172 seconds\n",
      "INFO:dedupe.blocking:610000, 44.0955772 seconds\n",
      "INFO:dedupe.blocking:620000, 44.7178762 seconds\n",
      "INFO:dedupe.blocking:630000, 45.3511592 seconds\n",
      "INFO:dedupe.blocking:640000, 46.0206562 seconds\n",
      "INFO:dedupe.blocking:650000, 46.7011252 seconds\n",
      "INFO:dedupe.blocking:660000, 47.3506552 seconds\n",
      "INFO:dedupe.blocking:670000, 47.9951442 seconds\n",
      "INFO:dedupe.blocking:680000, 48.6147162 seconds\n",
      "INFO:dedupe.blocking:690000, 49.2779072 seconds\n",
      "INFO:dedupe.blocking:700000, 49.9669462 seconds\n"
     ]
    }
   ],
   "source": [
    "with write_con:\n",
    "    with write_con.cursor() as cur:\n",
    "        cur.execute(\"DROP TABLE IF EXISTS blocking_map\")\n",
    "        cur.execute(\"CREATE TABLE blocking_map (block_key text, donor_id INTEGER)\")\n",
    "\n",
    "# If dedupe learned a Index Predicate, we have to take a pass\n",
    "# through the data and create indices.\n",
    "print('creating inverted index')\n",
    "\n",
    "for field in deduper.fingerprinter.index_fields:\n",
    "    with read_con.cursor('field_values') as cur:\n",
    "        cur.execute(\"SELECT DISTINCT %s FROM processed_donors\" % field)\n",
    "        field_data = (row[field] for row in cur)\n",
    "        deduper.fingerprinter.index(field_data, field)\n",
    "        \n",
    "# Now we are ready to write our blocking map table by creating a\n",
    "# generator that yields unique `(block_key, donor_id)` tuples.\n",
    "print('writing blocking map')\n",
    "with read_con.cursor('donor_select') as read_cur:\n",
    "    read_cur.execute(DONOR_SELECT)\n",
    "    full_data = ((row['donor_id'], row) for row in read_cur)\n",
    "    b_data = deduper.fingerprinter(full_data)\n",
    "    with write_con:\n",
    "        with write_con.cursor() as write_cur:\n",
    "            write_cur.copy_expert('COPY blocking_map FROM STDIN WITH CSV',\n",
    "                                  Readable(b_data),\n",
    "                                  size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct entities: 115782\n",
      "All references: 418995\n"
     ]
    }
   ],
   "source": [
    "with read_con: \n",
    "    with read_con.cursor() as cur:\n",
    "        cur.execute(\"SELECT COUNT(a) FROM (SELECT DISTINCT canon_id FROM entity_map) a\")\n",
    "        print(\"Distinct entities:\", cur.fetchone()['count'])\n",
    "        cur.execute(\"SELECT COUNT(*) FROM entity_map\")\n",
    "        print(\"All references:\", cur.fetchone()['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
